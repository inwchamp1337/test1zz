use crate::crawler::chrome_fetcher;

/// à¹‚à¸«à¸¡à¸”à¸à¸²à¸£à¹‚à¸«à¸¥à¸” HTML
#[derive(Debug, Clone, Copy)]
pub enum FetchMode {
    HttpRequest,
    Chrome,
}

impl FetchMode {
    pub fn from_str(s: &str) -> Self {
        match s {
            "Chrome" => FetchMode::Chrome,
            _ => FetchMode::HttpRequest,
        }
    }
}

/// à¹‚à¸«à¸¥à¸” HTML à¸ˆà¸²à¸ URLs à¹‚à¸”à¸¢à¹€à¸¥à¸·à¸­à¸à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ HttpRequest à¸«à¸£à¸·à¸­ Chrome (spider / spider_chrome)
/// - urls: à¸£à¸²à¸¢à¸à¸²à¸£ URL à¸—à¸µà¹ˆà¸ˆà¸°à¹‚à¸«à¸¥à¸”
/// - mode: FetchMode::HttpRequest à¸«à¸£à¸·à¸­ FetchMode::Chrome
/// - user_agent: user agent string
/// - delay_ms: delay à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¹à¸•à¹ˆà¸¥à¸° URL
pub async fn fetch_html_from_urls(
    urls: Vec<String>,
    mode: FetchMode,
    user_agent: &str,
    delay_ms: u64,
) -> Result<Vec<(String, String)>, Box<dyn std::error::Error>> {
    let mode_label = match mode {
        FetchMode::Chrome => "SPA (Chrome/JavaScript)",
        FetchMode::HttpRequest => "SSR (HttpRequest)",
    };
    println!("[html_fetcher] fetch_html_from_urls mode={:?} [{}] total_urls={}", mode, mode_label, urls.len());

    match mode {
        FetchMode::Chrome => {
            // à¹ƒà¸Šà¹‰ chrome_fetcher à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸«à¸¡à¸” Chrome
            println!("[html_fetcher] âš¡ SPA Mode - using fetch_with_chrome function");
            chrome_fetcher::fetch_with_chrome(urls, user_agent, delay_ms).await
        }
        FetchMode::HttpRequest => {
            // à¹ƒà¸Šà¹‰ HttpRequest à¹à¸šà¸šà¹€à¸”à¸´à¸¡à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸«à¸¡à¸” SSR
            println!("[html_fetcher] ğŸ“„ SSR Mode - using basic HTTP fetch (no JavaScript)");
            fetch_with_http_request(urls, user_agent, delay_ms).await
        }
    }
}

/// à¹‚à¸«à¸¥à¸” HTML à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ HttpRequest (à¸ªà¸³à¸«à¸£à¸±à¸š SSR)
async fn fetch_with_http_request(
    urls: Vec<String>,
    user_agent: &str,
    delay_ms: u64,
) -> Result<Vec<(String, String)>, Box<dyn std::error::Error>> {
    use std::time::Duration;
    use spider::website::Website;
    use spider::compact_str::CompactString;
    use tokio::time::sleep;

    let mut results = Vec::new();

    for url in urls {
        println!("[html_fetcher] start -> {}", url);

        let mut website = Website::new(&url);

        // à¸•à¸±à¹‰à¸‡ user-agent (Box<CompactString> à¸•à¸²à¸¡à¸—à¸µà¹ˆ spider à¸•à¹‰à¸­à¸‡à¸à¸²à¸£)
        website.configuration.user_agent = Some(Box::new(CompactString::new(user_agent)));

        // à¹‚à¸«à¸¥à¸”à¹à¸„à¹ˆà¸«à¸™à¹‰à¸²à¸™à¸±à¹‰à¸™ à¹†
        website.with_depth(0);

        // à¸•à¸±à¹‰à¸‡ delay à¸–à¹‰à¸²à¸¡à¸µ (spider configuration)
        website.configuration.delay = delay_ms;

        // Log internal configuration for visibility
        println!(
            "[html_fetcher] config -> user_agent={:?}, delay_ms={}, depth={}",
            website.configuration.user_agent.as_ref().map(|b| b.as_ref()),
            website.configuration.delay,
            website.configuration.depth
        );

        // à¹€à¸£à¸µà¸¢à¸ scrape / crawl (spider API) â€” à¹ƒà¸Šà¹‰ await
        let t0 = std::time::Instant::now();
        println!("[html_fetcher] scrape start: {}", url);
        website.scrape().await;
        let took = t0.elapsed();
        println!("[html_fetcher] scrape done: {} (took {:?})", url, took);

        // à¸à¸´à¸¡à¸à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ pages à¸—à¸µà¹ˆà¹„à¸”à¹‰ (debug)
        if let Some(pages) = website.get_pages() {
            println!("[html_fetcher] pages returned: {}", pages.len());
            for (i, page) in pages.iter().enumerate() {
                println!("  [page {}] url={} (html_len={})", i + 1, page.get_url(), page.get_html().len());
            }
            if let Some(page) = pages.first() {
                let html = page.get_html().to_string();
                println!("[html_fetcher] fetched {} bytes from {}", html.len(), url);
                results.push((url.clone(), html));
            } else {
                eprintln!("[html_fetcher] no page for url: {}", url);
            }
        } else {
            eprintln!("[html_fetcher] get_pages returned None for url: {}", url);
        }

        // delay à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ requests
        if delay_ms > 0 {
            sleep(Duration::from_millis(delay_ms)).await;
        }
    }

    println!("[html_fetcher] finished, got {} pages", results.len());
    Ok(results)
}